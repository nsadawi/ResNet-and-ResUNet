{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgo5pGCuq83O"
   },
   "source": [
    "# THE PROBLEM STATEMENT AND BUSINESS CASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40uh1KD1sp6O"
   },
   "source": [
    "# PART 1. FACIAL KEY POINTS DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVTymJearEJn"
   },
   "source": [
    "# IMPORT LIBRARIES AND DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gkdq6xgwrHD8",
    "outputId": "31d08917-5141-4491-f7d3-1fd15e7a5934"
   },
   "outputs": [],
   "source": [
    "%cd Emotion+AI+Dataset/Emotion AI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQFbssqfr_UL"
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from PIL import *\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from IPython.display import display\n",
    "from tensorflow.python.keras import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klj2F-OQr_Wn"
   },
   "outputs": [],
   "source": [
    "# load facial key points data\n",
    "keyfacial_df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xTb3fLL8r_ZQ",
    "outputId": "f34231e5-ca55-4de9-e3af-5629eb0478b4"
   },
   "outputs": [],
   "source": [
    "keyfacial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yFsN8SAtHum",
    "outputId": "72cdb29b-bedc-45c3-f1f7-58132799407b"
   },
   "outputs": [],
   "source": [
    "# Obtain relavant information about the dataframe\n",
    "keyfacial_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image is initially a string\n",
    "#keyfacial_df['Image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJhdlYRqtKVK",
    "outputId": "d8348f1b-00c1-4a40-b295-9267ef723710"
   },
   "outputs": [],
   "source": [
    "# Check if null values exist in the dataframe\n",
    "keyfacial_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-s17cxKBvd3",
    "outputId": "e1426f74-a16f-457a-ea05-25a71891498c"
   },
   "outputs": [],
   "source": [
    "keyfacial_df['Image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtGMT3ozr_g3"
   },
   "outputs": [],
   "source": [
    "# Since values for the image are given as space separated string, separate the values using ' ' as separator.\n",
    "# Then convert this into numpy array using np.fromstring and convert the obtained 1D array into 2D array of shape (96, 96)\n",
    "keyfacial_df['Image'] = keyfacial_df['Image'].apply(lambda x: np.fromstring(x, dtype = int, sep = ' ').reshape(96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE1RIIy8r_fN",
    "outputId": "a5a0f621-69e2-4bfe-b570-0682b2258e8c"
   },
   "outputs": [],
   "source": [
    "# Obtain the Shape of the image\n",
    "keyfacial_df['Image'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "06r0LqnKflxU",
    "outputId": "60902d54-e7af-4f1e-c97b-a2fe860e3bd6"
   },
   "outputs": [],
   "source": [
    "keyfacial_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNkYttG_tOKf"
   },
   "source": [
    "# PERFORM IMAGE VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "KpJujwjyr_dc",
    "outputId": "28bdf87e-5988-4211-e2a5-5d0dd0d14b77"
   },
   "outputs": [],
   "source": [
    "# Plot a random image from the dataset along with facial keypoints. \n",
    "# Image data is obtained from df['Image'] and plotted using plt.imshow\n",
    "# 15 x and y coordinates for the corresponding image \n",
    "# since x-coordinates are in even columns like 0,2,4,.. and y-coordinates are in odd columns like 1,3,5,..\n",
    "# we access their value using .loc command, which get the values for coordinates of the image based on the column it is refering to.\n",
    "\n",
    "i = np.random.randint(1, len(keyfacial_df))\n",
    "plt.imshow(keyfacial_df['Image'][i], cmap = 'gray')\n",
    "for j in range(1, 31, 2):\n",
    "        plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], 'rx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "oSZfQwc3r_be",
    "outputId": "cab91986-4b52-4363-b121-08599dfb2763"
   },
   "outputs": [],
   "source": [
    "# Let's view more images in a grid format\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)    \n",
    "    image = plt.imshow(keyfacial_df['Image'][i],cmap = 'gray')\n",
    "    for j in range(1,31,2):\n",
    "        plt.plot(keyfacial_df.loc[i][j-1], keyfacial_df.loc[i][j], 'rx')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 971
    },
    "id": "tPbtjUqHhYTX",
    "outputId": "f1ec8458-2a61-4cb5-d25b-3a8c3e0e0550"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Let's view more images in a grid format\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(64):\n",
    "    k = random.randint(1, len(keyfacial_df))\n",
    "    ax = fig.add_subplot(8, 8, i + 1)    \n",
    "    image = plt.imshow(keyfacial_df['Image'][k],cmap = 'gray')\n",
    "    for j in range(1,31,2):\n",
    "        plt.plot(keyfacial_df.loc[k][j-1], keyfacial_df.loc[k][j], 'rx')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbqDwd1mteJ4"
   },
   "source": [
    "# PERFORM IMAGE AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3TLq1UbtazX"
   },
   "outputs": [],
   "source": [
    "# Create a new copy of the dataframe\n",
    "import copy\n",
    "keyfacial_df_copy = copy.copy(keyfacial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "ypyn10X7tbrb",
    "outputId": "f82352c3-09e3-410f-8adc-a4b5236e1a1e"
   },
   "outputs": [],
   "source": [
    "# Obtain the columns in the dataframe\n",
    "\n",
    "columns = keyfacial_df_copy.columns[:-1]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyFb3o1ztbyr"
   },
   "outputs": [],
   "source": [
    "# Horizontal Flip - flip the images along y axis\n",
    "keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 1))\n",
    "\n",
    "# since we are flipping horizontally, y coordinate values would be the same\n",
    "# Only x coordiante values would change, all we have to do is to subtract our initial x-coordinate values from width of the image(96)\n",
    "for i in range(len(columns)):\n",
    "    if i%2 == 0:\n",
    "        keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "fIM1786rtb37",
    "outputId": "d94ebc0e-3be7-4b24-e256-ffaf0047bf1b"
   },
   "outputs": [],
   "source": [
    "# Show the Original image\n",
    "plt.imshow(keyfacial_df['Image'][0], cmap = 'gray')\n",
    "for j in range(1, 31, 2):\n",
    "        plt.plot(keyfacial_df.loc[0][j-1], keyfacial_df.loc[0][j], 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "1kVUp-UMtbwg",
    "outputId": "2222ecba-1531-49fd-9454-f5fb901f039d"
   },
   "outputs": [],
   "source": [
    "# Show the Horizontally flipped image\n",
    "plt.imshow(keyfacial_df_copy['Image'][0],cmap='gray')\n",
    "for j in range(1, 31, 2):\n",
    "        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g67tKjwtbua"
   },
   "outputs": [],
   "source": [
    "# Concatenate the original dataframe with the augmented dataframe\n",
    "augmented_df = np.concatenate((keyfacial_df, keyfacial_df_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "IRag8bteDzVl",
    "outputId": "77c84ab4-7f33-464c-f528-198580bec335"
   },
   "outputs": [],
   "source": [
    "augmented_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "COzWKpthtvRW",
    "outputId": "e2288d12-8506-4e09-fd40-3cab2e82dcfe"
   },
   "outputs": [],
   "source": [
    "# Randomingly increasing the brightness of the images\n",
    "# We multiply pixel values by random values between 1.5 and 2 to increase the brightness of the image\n",
    "# we clip the value between 0 and 255\n",
    "\n",
    "import random\n",
    "\n",
    "keyfacial_df_copy = copy.copy(keyfacial_df)\n",
    "keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x:np.clip(random.uniform(1.5, 2)* x, 0.0, 255.0))\n",
    "augmented_df = np.concatenate((augmented_df, keyfacial_df_copy))\n",
    "augmented_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "S_Pvd2qqtvkF",
    "outputId": "718ed0b2-9ed4-4243-d04e-49eb654373d8"
   },
   "outputs": [],
   "source": [
    "# Show Image with increased brightness\n",
    "\n",
    "plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')\n",
    "for j in range(1, 31, 2):\n",
    "        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mm0qhrGNdcY9"
   },
   "source": [
    "#### Augment images by flipping them vertically \n",
    "(Flip along x-axis and note that if we are flipping along x-axis, x co-ordinates won't change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQV4Lwr6dkmK"
   },
   "outputs": [],
   "source": [
    "# keyfacial_df_copy = copy.copy(keyfacial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X607yQkqCBd9"
   },
   "outputs": [],
   "source": [
    "# keyfacial_df_copy['Image'] = keyfacial_df_copy['Image'].apply(lambda x: np.flip(x, axis = 0))\n",
    "\n",
    "# for i in range(len(columns)):\n",
    "#  if i%2 == 1:\n",
    "#    keyfacial_df_copy[columns[i]] = keyfacial_df_copy[columns[i]].apply(lambda x: 96. - float(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "ERhDcD2XiP4u",
    "outputId": "79f474ee-fa7d-47d2-c7f5-8b9e8d067e0f"
   },
   "outputs": [],
   "source": [
    "# Perform a sanity check and visualize sample images\n",
    "\n",
    "#plt.imshow(keyfacial_df_copy['Image'][0], cmap='gray')\n",
    "#for j in range(1, 31, 2):\n",
    "#        plt.plot(keyfacial_df_copy.loc[0][j-1], keyfacial_df_copy.loc[0][j], 'rx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mv4RFiu4iP2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz56c0e0t71Y"
   },
   "source": [
    "# PERFORM DATA NORMALIZATION AND TRAINING DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sNdOOb0Ctvei",
    "outputId": "d63638c6-e303-459c-96f0-e6ae8ccfe14d"
   },
   "outputs": [],
   "source": [
    "# Obtain the value of images which is present in the 31st column (since index start from 0, we refer to 31st column by 30)\n",
    "img = augmented_df[:,30]\n",
    "\n",
    "# Normalize the images\n",
    "img = img/255.\n",
    "\n",
    "# Create an empty array of shape (x, 96, 96, 1) to feed the model\n",
    "X = np.empty((len(img), 96, 96, 1))\n",
    "\n",
    "# Iterate through the img list and add image values to the empty array after expanding it's dimension from (96, 96) to (96, 96, 1)\n",
    "for i in range(len(img)):\n",
    "    X[i,] = np.expand_dims(img[i], axis = 2)\n",
    "\n",
    "# Convert the array type to float32\n",
    "X = np.asarray(X).astype(np.float32)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "hPNN-RKftvcF",
    "outputId": "3e810284-1c1d-455b-e6a1-cbca2d4d7cd1"
   },
   "outputs": [],
   "source": [
    "# Obtain the value of x & y coordinates which are to used as target.\n",
    "y = augmented_df[:,:30]\n",
    "y = np.asarray(y).astype(np.float32)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl7FHUUMtvZr"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rQmZG5fxkU9d",
    "outputId": "1e6bcdbf-811b-4ed0-be18-1232707447ca"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xe_63C3PFiIC",
    "outputId": "4db2f8e7-fdf2-486c-eb70-dc35ccc2e9f3"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8I3pVH-ks1a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIXkiaHBuNrg"
   },
   "source": [
    "# BUILD DEEP RESIDUAL NEURAL NETWORK KEY FACIAL POINTS DETECTION MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lCAPjVotvXi"
   },
   "outputs": [],
   "source": [
    "def res_block(X, filter, stage):\n",
    "\n",
    "    # Convolutional_block\n",
    "    X_copy = X\n",
    "\n",
    "    f1, f2, f3 = filter\n",
    "\n",
    "    # Main Path\n",
    "    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = MaxPool2D((2,2))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n",
    "\n",
    "\n",
    "    # Short path\n",
    "    X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n",
    "    X_copy = MaxPool2D((2,2))(X_copy)\n",
    "    X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n",
    "\n",
    "    # ADD\n",
    "    X = Add()([X,X_copy])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Identity Block 1\n",
    "    X_copy = X\n",
    "\n",
    "\n",
    "    # Main Path\n",
    "    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n",
    "\n",
    "    # ADD\n",
    "    X = Add()([X,X_copy])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Identity Block 2\n",
    "    X_copy = X\n",
    "\n",
    "\n",
    "    # Main Path\n",
    "    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n",
    "\n",
    "    # ADD\n",
    "    X = Add()([X,X_copy])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NB9dPqlTtvVH",
    "outputId": "69199dfe-415c-4161-d598-39a5f1e71d6e"
   },
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 1)\n",
    "\n",
    "# Input tensor shape\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# Zero-padding\n",
    "X = ZeroPadding2D((3,3))(X_input)\n",
    "\n",
    "# 1 - stage\n",
    "X = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3,3), strides= (2,2))(X)\n",
    "\n",
    "# 2 - stage\n",
    "X = res_block(X, filter= [64,64,256], stage= 2)\n",
    "\n",
    "# 3 - stage\n",
    "X = res_block(X, filter= [128,128,512], stage= 3)\n",
    "\n",
    "\n",
    "# Average Pooling\n",
    "X = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n",
    "\n",
    "# Final layer\n",
    "X = Flatten()(X)\n",
    "X = Dense(4096, activation = 'relu')(X)\n",
    "X = Dropout(0.2)(X)\n",
    "X = Dense(2048, activation = 'relu')(X)\n",
    "X = Dropout(0.1)(X)\n",
    "X = Dense(30, activation = 'relu')(X)\n",
    "\n",
    "\n",
    "model_1_facialKeyPoints = Model( inputs= X_input, outputs = X)\n",
    "model_1_facialKeyPoints.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEND_oH7mANd"
   },
   "source": [
    "#### If you wish:\n",
    "- Experiment with changing the network architecture by removing 2 MaxPooling layers from the Res Block and train the model\n",
    "- Try to add 'X = res_block(X, filter= [256,256,1024], stage= 4)' Block after stage #3 block. \n",
    "- What did you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6yZkdjLeFnN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJCLhas0ulnq"
   },
   "source": [
    "# COMPILE AND TRAIN KEY FACIAL POINTS DETECTION DEEP LEARNING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mr662N98upOP"
   },
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad = False)\n",
    "model_1_facialKeyPoints.compile(loss = \"mean_squared_error\", optimizer = adam , metrics = ['accuracy'])\n",
    "# Check this out for more information on Adam optimizer: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pj-gfDCusny"
   },
   "outputs": [],
   "source": [
    "# save the best model with least validation loss\n",
    "# It will save the model weights\n",
    "checkpointer = ModelCheckpoint(filepath = \"FacialKeyPoints_weights.hdf5\", verbose = 1, save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Can take a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "IvWHFJyousme",
    "outputId": "e7904147-7520-4a38-b647-6d9de2f7203f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#### two epochs took about 3 minutes on my machine\n",
    "#history = model_1_facialKeyPoints.fit(X_train, y_train, batch_size = 32, epochs = 2, validation_split = 0.05, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VLk1ZKPusjM"
   },
   "outputs": [],
   "source": [
    "# save the model architecture to json file for future use\n",
    "\n",
    "#model_json = model_1_facialKeyPoints.to_json()\n",
    "#with open(\"FacialKeyPoints-model.json\",\"w\") as json_file:\n",
    "#    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_p7ramEeMGa"
   },
   "source": [
    "- You can experiment with changing the batch size and validation split value and retrain the model (Take Home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GynkMge9eNd_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-cVW8K62Pai"
   },
   "source": [
    "# ASSESS TRAINED KEY FACIAL POINTS DETECTION MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iGWmYbdu2mF"
   },
   "outputs": [],
   "source": [
    "# Pre-saved model architecture\n",
    "with open('detection.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "    \n",
    "# load the model architecture \n",
    "model_1_facialKeyPoints = tf.keras.models.model_from_json(json_savedModel)\n",
    "# load the model weights\n",
    "model_1_facialKeyPoints.load_weights('weights_keypoint.hdf5')\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model_1_facialKeyPoints.compile(loss=\"mean_squared_error\", optimizer= adam , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-XuTiBexu2j5",
    "outputId": "24e18055-a006-442b-9cee-6b6910c15bf3"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "result = model_1_facialKeyPoints.evaluate(X_test, y_test)\n",
    "print(\"Accuracy : {}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "NA-LZmf-u2iE",
    "outputId": "e887257c-3129-45fd-85ca-cd05821dc8c0"
   },
   "outputs": [],
   "source": [
    "# Get the model keys \n",
    "#history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJfpDL2xu2cV"
   },
   "outputs": [],
   "source": [
    "# Plot the training artifacts\n",
    "\n",
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "#plt.title('Model loss')\n",
    "#plt.ylabel('loss')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train_loss','val_loss'], loc = 'upper right')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvTWqbOI2tty"
   },
   "source": [
    "# PART 2. FACIAL EXPRESSION DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIKg0ZXk3DWl"
   },
   "source": [
    "# IMPORT & EXPLORE DATASET FOR FACIAL EXPRESSION DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_NT8ASSu2aT"
   },
   "outputs": [],
   "source": [
    "# read the csv files for the facial expression data\n",
    "facialexpression_df = pd.read_csv('icml_face_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "CTiebNB2u2YP",
    "outputId": "2adaef39-dbea-46f4-8297-99e5ce65fe49"
   },
   "outputs": [],
   "source": [
    "facialexpression_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "id": "yGIesU_F0AMm",
    "outputId": "76754a0e-4d69-482b-8e97-7f60738d57cf"
   },
   "outputs": [],
   "source": [
    "facialexpression_df[' pixels'][0] # String format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMLQbIEh3Ma6"
   },
   "outputs": [],
   "source": [
    "# function to convert pixel values in string format to array format\n",
    "\n",
    "def string2array(x):\n",
    "    return np.array(x.split(' ')).reshape(48, 48, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkUHXFoU3MYq"
   },
   "outputs": [],
   "source": [
    "# Resize images from (48, 48) to (96, 96)\n",
    "\n",
    "def resize(x):\n",
    "    img = x.reshape(48, 48)\n",
    "    return cv2.resize(img, dsize=(96, 96), interpolation = cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GexILbbK3MWK"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: string2array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZr0wp553MUp"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "facialexpression_df[' pixels'] = facialexpression_df[' pixels'].apply(lambda x: resize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "2Rbv7ZyC3MSM",
    "outputId": "f63a490d-7b87-4a03-aa70-3ead4ef60674"
   },
   "outputs": [],
   "source": [
    "facialexpression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gkreBNCT3MPu",
    "outputId": "7c2d0e5a-8417-4b95-aa83-b000988c7de3"
   },
   "outputs": [],
   "source": [
    "# check the shape of data_frame\n",
    "facialexpression_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "gkQQp5gy3MNa",
    "outputId": "6c81a7fe-e24a-403a-a6b7-876eeeddbb8f"
   },
   "outputs": [],
   "source": [
    "# check for the presence of null values in the data frame\n",
    "facialexpression_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-BzW6jL3MK0"
   },
   "outputs": [],
   "source": [
    "label_to_text = {0:'anger', 1:'disgust', 2:'sad', 3:'happiness', 4: 'surprise'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ceohXF_1QIk"
   },
   "source": [
    "- Visualize the first image in the dataframe and make sure that the image is not distorted by resizing or reshaping operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "_xm9oL5k1ED6",
    "outputId": "08f5e158-931e-4092-99e1-2a1954bf251e"
   },
   "outputs": [],
   "source": [
    "plt.imshow(facialexpression_df[' pixels'][30], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JtBq4Ys3m5A"
   },
   "source": [
    "# VISUALIZE IMAGES AND PLOT LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tZmIbWvE3rJM",
    "outputId": "53e3c424-1954-448b-ab66-c445be8a31a0"
   },
   "outputs": [],
   "source": [
    "emotions = [0, 1, 2, 3, 4]\n",
    "\n",
    "for i in emotions:\n",
    "  data = facialexpression_df[facialexpression_df['emotion'] == i][:1]\n",
    "  img = data[' pixels'].item()\n",
    "  img = img.reshape(96, 96)\n",
    "  plt.figure()\n",
    "  plt.title(label_to_text[i])\n",
    "  plt.imshow(img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQj1t3mt1t7_"
   },
   "source": [
    "- Plot bar chart to outline how many samples (images) are present per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Mqd57PASp_tl",
    "outputId": "de84107a-d2eb-4098-cd4a-8ac14fc15eed"
   },
   "outputs": [],
   "source": [
    "facialexpression_df.emotion.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "bMnwhXw_qGyq",
    "outputId": "765406ad-1af1-44a1-8c14-a3b9ef35ac3d"
   },
   "outputs": [],
   "source": [
    "facialexpression_df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "oxAbaC1g3pNP",
    "outputId": "5134bece-e7f5-469a-ab96-fe39bbd0a379"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.barplot(x = facialexpression_df.emotion.value_counts().index, y = facialexpression_df.emotion.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ib_YTklF3wGy"
   },
   "source": [
    "# PERFORM DATA PREPARATION AND IMAGE AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgzxfltG3rHt"
   },
   "outputs": [],
   "source": [
    "# split the dataframe in to features and labels\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "X = facialexpression_df[' pixels']\n",
    "y = to_categorical(facialexpression_df['emotion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "gx91bryj0zu_",
    "outputId": "eaba1e27-665d-440c-b22c-559b997a7351"
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "WNDkM9ZL0-kP",
    "outputId": "7aaa8efc-6dc1-4084-8478-c014753258ce"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pfPslLye0wgZ",
    "outputId": "62728d77-a9bc-44dd-b005-9fe1779214fc"
   },
   "outputs": [],
   "source": [
    "# Update the shape of X to become (num images, w, h) \n",
    "X = np.stack(X, axis = 0)\n",
    "\n",
    "# Update the shape of X to become (num images, w, h, 1) .. i.e. to become a Tensor\n",
    "X = X.reshape(24568, 96, 96, 1)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1zcH-d83rGF"
   },
   "outputs": [],
   "source": [
    "# split the dataframe in to train, test and validation data frames\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_Test, y_train, y_Test = train_test_split(X, y, test_size = 0.1, shuffle = True)\n",
    "X_val, X_Test, y_val, y_Test = train_test_split(X_Test, y_Test, test_size = 0.5, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kkLQt43f3rE2",
    "outputId": "4e0537e0-0d96-4d03-8f09-65313f848084"
   },
   "outputs": [],
   "source": [
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Febj9BnO1xaA",
    "outputId": "26a469f2-b9d8-4f6b-bd98-37d56624ad0c"
   },
   "outputs": [],
   "source": [
    "print(X_Test.shape, y_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YDH4-6h211dj",
    "outputId": "c4a24b71-7cf3-4cef-c65d-62a38adb5bfd"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDAFSti14MUD"
   },
   "outputs": [],
   "source": [
    "# image pre-processing\n",
    "\n",
    "X_train = X_train/255\n",
    "X_val   = X_val /255\n",
    "X_Test  = X_Test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Wh8hASOZ1_GG",
    "outputId": "b3241255-89ac-41dd-9db8-9ad0f33464e8"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IgiiGW64MRm"
   },
   "outputs": [],
   "source": [
    "# Perform image augmentation ob the fly\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "# https://keras.io/api/preprocessing/image/\n",
    "train_datagen = ImageDataGenerator(\n",
    "rotation_range = 15,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.1,\n",
    "    zoom_range = 0.1,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = \"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NyS_hKR3j_I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5yyM1UF4Xv2"
   },
   "source": [
    "# BUILD AND TRAIN DEEP LEARNING MODEL FOR FACIAL EXPRESSION CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5iYII6C24MQD",
    "outputId": "1fcb68f3-8b7e-4257-ded2-432beb1d9cbe"
   },
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 1)\n",
    "\n",
    "# Input tensor shape\n",
    "X_input = Input(input_shape)\n",
    "\n",
    "# Zero-padding\n",
    "X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "# 1 - stage\n",
    "X = Conv2D(64, (7, 7), strides= (2, 2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\n",
    "X = BatchNormalization(axis =3, name = 'bn_conv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((3, 3), strides= (2, 2))(X)\n",
    "\n",
    "# 2 - stage\n",
    "X = res_block(X, filter= [64, 64, 256], stage= 2)\n",
    "\n",
    "# 3 - stage\n",
    "X = res_block(X, filter= [128, 128, 512], stage= 3)\n",
    "\n",
    "# 4 - stage\n",
    "# X = res_block(X, filter= [256, 256, 1024], stage= 4)\n",
    "\n",
    "# Average Pooling\n",
    "X = AveragePooling2D((4, 4), name = 'Averagea_Pooling')(X)\n",
    "\n",
    "# Final layer\n",
    "X = Flatten()(X)\n",
    "X = Dense(5, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n",
    "\n",
    "model_2_emotion = Model( inputs= X_input, outputs = X, name = 'Resnet18')\n",
    "\n",
    "model_2_emotion.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SHnD2aX4MLJ"
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "model_2_emotion.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ubSY9CQ4MJG"
   },
   "outputs": [],
   "source": [
    "# Recall that the first facial key points model was saved as follows: FacialKeyPoints_weights.hdf5 and FacialKeyPoints-model.json\n",
    "\n",
    "# using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)\n",
    "\n",
    "# save the best model with lower validation loss\n",
    "checkpointer = ModelCheckpoint(filepath = \"FacialExpression_weights.hdf5\", verbose = 1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two epochs took about 8 minutes on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "iqjpKgHK3q_n",
    "outputId": "72843016-3f28-4693-c89b-3660d9317523"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#history = model_2_emotion.fit(train_datagen.flow(X_train, y_train, batch_size=64),\n",
    "#                              validation_data=(X_val, y_val), steps_per_epoch=len(X_train) // 64,\n",
    "#                              epochs= 2, callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJe9up3A4zUI"
   },
   "outputs": [],
   "source": [
    "# saving the model architecture to json file for future use\n",
    "\n",
    "#model_json = model_2_emotion.to_json()\n",
    "#with open(\"FacialExpression-model.json\",\"w\") as json_file:\n",
    "#    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uS71oV-j2ZXh"
   },
   "source": [
    "- Experiment with various batch size, patience, optimizers, and network architecture to improve network performance (Take home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MREPeVkF2Y3i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LxpMt2h5AIi"
   },
   "source": [
    "# ASSESS THE PERFORMANCE OF TRAINED FACIAL EXPRESSION CLASSIFIER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKtc94LP4zSj"
   },
   "outputs": [],
   "source": [
    "with open('emotion.json', 'r') as json_file:\n",
    "    json_savedModel= json_file.read()\n",
    "    \n",
    "# load the model architecture \n",
    "model_2_emotion = tf.keras.models.model_from_json(json_savedModel)\n",
    "model_2_emotion.load_weights('weights_emotions.hdf5')\n",
    "model_2_emotion.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "NJoYkZWw43zv",
    "outputId": "18720dad-ec90-44a3-ac4a-bebe08198e67"
   },
   "outputs": [],
   "source": [
    "score = model_2_emotion.evaluate(X_Test, y_Test)\n",
    "print('Test Accuracy: {}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "M27CZcLm4zQk",
    "outputId": "ffb0aff4-e1a5-4d77-e187-f0bd3f7d082d"
   },
   "outputs": [],
   "source": [
    "#history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2WZ3__e5Kxp"
   },
   "outputs": [],
   "source": [
    "#accuracy = history.history['accuracy']\n",
    "#val_accuracy = history.history['val_accuracy']\n",
    "#loss = history.history['loss']\n",
    "#val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "sEO2vCRh5Kpz",
    "outputId": "071744c3-9fa9-4f85-9f52-518a83f507ee"
   },
   "outputs": [],
   "source": [
    "#epochs = range(len(accuracy))\n",
    "\n",
    "#plt.plot(epochs, accuracy, 'bo', label='Training Accuracy')\n",
    "#plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
    "#plt.title('Training and Validation Accuracy')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "k9WS5-Bv5KoD",
    "outputId": "d43125bb-9b5e-42fa-999f-592569dd4900"
   },
   "outputs": [],
   "source": [
    "#plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "#plt.title('Training and Validation loss')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oduf0Q8n5Kl-"
   },
   "outputs": [],
   "source": [
    "# predicted_classes = model.predict_classes(X_test)\n",
    "predicted_classes = np.argmax(model_2_emotion.predict(X_Test), axis=-1)\n",
    "y_true = np.argmax(y_Test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "E5Yp58fm5Kkj",
    "outputId": "2bb18ef5-32e2-4604-af55-5c320fb0f8b6"
   },
   "outputs": [],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "cLdhjhBb5KeD",
    "outputId": "152d627c-a65d-4ec2-f00a-8bb9b10d26bb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, predicted_classes)\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(cm, annot = True, cbar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTQGje8f6ug0"
   },
   "source": [
    "- Print out a grid of 25 images along with their predicted/true label\n",
    "- Print out the classification report and analyze precision and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YuLsavNIGQqA",
    "outputId": "9b59d259-929d-43a1-c53c-942112a56072"
   },
   "outputs": [],
   "source": [
    "L = 5\n",
    "W = 5\n",
    "\n",
    "fig, axes = plt.subplots(L, W, figsize = (24, 24))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, L*W):\n",
    "    axes[i].imshow(X_test[i].reshape(96,96), cmap = 'gray')\n",
    "    axes[i].set_title('Prediction = {}\\n True = {}'.format(label_to_text[predicted_classes[i]], label_to_text[y_true[i]]))\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace = 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "2yT4LdSCGuDJ",
    "outputId": "e92e5e69-eebe-4501-ab46-58a33b734430"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Gm-Vap7Wv0"
   },
   "source": [
    "# PART 3. COMBINE BOTH FACIAL EXPRESSION AND KEY POINTS DETECTION MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_kdiO4o7IPc"
   },
   "source": [
    "# COMBINE BOTH MODELS (1) FACIAL KEY POINTS DETECTION AND (2) FACIAL EXPRESSION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDVsSChW7Vpi"
   },
   "outputs": [],
   "source": [
    "def predict(X_test):\n",
    "\n",
    "    # Making prediction from the keypoint model\n",
    "    df_predict = model_1_facialKeyPoints.predict(X_test)\n",
    "\n",
    "    # Making prediction from the emotion model\n",
    "    df_emotion = np.argmax(model_2_emotion.predict(X_test), axis=-1)\n",
    "\n",
    "    # Reshaping array from (856,) to (856,1)\n",
    "    df_emotion = np.expand_dims(df_emotion, axis = 1)\n",
    "\n",
    "    # Converting the predictions into a dataframe\n",
    "    df_predict = pd.DataFrame(df_predict, columns= columns)\n",
    "\n",
    "    # Adding emotion into the predicted dataframe\n",
    "    df_predict['emotion'] = df_emotion\n",
    "\n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULEWfmhK7vVf"
   },
   "outputs": [],
   "source": [
    "df_predict = predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "9g1M3Hrq7vO-",
    "outputId": "72b30ef4-602a-4ac0-d197-7098edd51cbb"
   },
   "outputs": [],
   "source": [
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmhAnDXf7HpU"
   },
   "source": [
    "- Plot a grid of 16 images along with their predicted emotion and facial key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7Ogvh7t-JhhM",
    "outputId": "293cb546-c838-4bff-ef26-e776cc0abd77"
   },
   "outputs": [],
   "source": [
    "# Plotting the test images and their predicted keypoints and emotions\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize = (24, 24))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(16):\n",
    "\n",
    "    axes[i].imshow(X_test[i].squeeze(),cmap='gray')\n",
    "    axes[i].set_title('Prediction = {}'.format(label_to_text[df_predict['emotion'][i]]))\n",
    "    axes[i].axis('off')\n",
    "    for j in range(1,31,2):\n",
    "            axes[i].plot(df_predict.loc[i][j-1], df_predict.loc[i][j], 'rx')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHxVJ2Ei8QDP"
   },
   "source": [
    "# WELL DONE!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion AI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
